{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cbe3b82910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "train_data = np.loadtxt('./train.csv', delimiter=',', dtype=str, skiprows=1)\n",
    "#label = data\n",
    "#set_label = set(label)\n",
    "#print(data)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load one-hot encoding set\n",
    "symptom_set = disease_set = []\n",
    "with open('ohe_symptoms.txt', 'r') as ohe:\n",
    "    symptom_set = ohe.readlines()\n",
    "symptom_set = [s[:-1] for s in symptom_set]\n",
    "\n",
    "with open('ohe_diseases.txt', 'r') as ohe:\n",
    "    disease_set = ohe.readlines()\n",
    "disease_set = [d[:-1] for d in disease_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#131 possible symptoms\n",
    "#41 diseases\n",
    "#print(data)\n",
    "print(symptom_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train_data(X):\n",
    "    N,d = X.shape\n",
    "\n",
    "    symptoms = X[:,1:]\n",
    "    set_symptom = set(symptoms.flatten())\n",
    "    set_symptom.remove('')\n",
    "    symptoms = list(set_symptom)\n",
    "\n",
    "    i = 0\n",
    "    for s in symptoms:\n",
    "        if s[0] == ' ':\n",
    "            symptoms[i] = symptoms[i][1:]\n",
    "        i += 1\n",
    "\n",
    "    diseases = X[:,0]\n",
    "    set_diseases = set(diseases)\n",
    "    diseases = list(set_diseases)\n",
    "\n",
    "    trans_input = np.zeros(shape=(N, 131), dtype=int)\n",
    "    trans_label = np.zeros(shape=(N,1), dtype=int)\n",
    "    for i in range(N):\n",
    "        for sym in X[i,1:]:\n",
    "            if sym != '':\n",
    "                s = sym\n",
    "                if s[0] == ' ':\n",
    "                    s = s[1:]\n",
    "                trans_input[i,symptoms.index(s)] = 1\n",
    "        trans_label[i,0] = diseases.index(X[i,0])\n",
    "    return trans_input, trans_label, symptoms, diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode data\n",
    "t_data, t_label, symp, dis = transform_train_data(train_data)\n",
    "train_data, train_label = t_data, t_label\n",
    "\n",
    "#validation/train split\n",
    "#valid_rows = np.random.choice(len(t_data),size=40, replace=False)\n",
    "#valid_data, valid_label = t_data[valid_rows,:], t_label[valid_rows,:]\n",
    "#train_data, train_label = np.delete(t_data, valid_rows, axis=0), np.delete(t_label, valid_rows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save one-hot encoding\n",
    "with open('ohe_symptoms.txt', 'w') as ohe:\n",
    "    ohe.writelines('%s\\n' % s for s in symp)\n",
    "with open('ohe_diseases.txt', 'w') as ohe:\n",
    "    ohe.writelines('%s\\n' % d for d in dis)\n",
    "#np.savetxt('ohe_symptoms.csv', np.array(symp), fmt='%s')\n",
    "#np.savetxt('ohe_diseases.csv', np.array(dis), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Running on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, num_dis, num_symp):\n",
    "        super(NN, self).__init__()\n",
    "        self.num_dis = num_dis\n",
    "        self.num_symp = num_symp\n",
    "        self.ll1 = nn.Linear(num_symp, 512)\n",
    "        self.ll2 = nn.Linear(512, 512)\n",
    "        self.ll3 = nn.Linear(512, 256)\n",
    "        self.ll4 = nn.Linear(256, 512)\n",
    "        self.ll5 = nn.Linear(512, 256)\n",
    "        self.ll6 = nn.Linear(256, 128)\n",
    "        self.ll5 = nn.Linear(128, num_dis)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ll1(x))\n",
    "        x = F.relu(self.ll2(x))\n",
    "        x = F.relu(self.ll3(x))\n",
    "        x = F.relu(self.ll4(x))\n",
    "        x = F.relu(self.ll5(x))\n",
    "        x = F.relu(self.ll6(x))\n",
    "        x = self.ll7(x)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CS145_NN(\n",
       "  (ll1): Linear(in_features=131, out_features=512, bias=True)\n",
       "  (ll2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (ll5): Linear(in_features=128, out_features=41, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model\n",
    "from model import CS145_NN\n",
    "model_d = CS145_NN()\n",
    "model_d.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "#Optimizer\n",
    "optim = torch.optim.Adam(model_d.parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input, label, model, batch_size, loss_func, optimizer, device):\n",
    "    input, label = torch.tensor(input).to(device).float(), torch.tensor(label).to(device).long()\n",
    "    model = model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    model.train()\n",
    "    n_batch = int(len(label)/batch_size)\n",
    "    if n_batch * batch_size < len(label):\n",
    "        n_batch += 1\n",
    "\n",
    "    \n",
    "    for b in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "        inp = input[b*batch_size:(b+1)*batch_size]\n",
    "        lab = label[b*batch_size:(b+1)*batch_size]\n",
    "        pred = model(inp)\n",
    "        loss = loss_func(pred, lab.flatten())\n",
    "        loss.backward()\n",
    "        losses.append(loss)\n",
    "        optimizer.step()\n",
    "    print(f'End of Epoch Loss: {round((sum(losses)/len(input)).item(), 3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "def test(input, label, model, device):\n",
    "    model = model.to(device)\n",
    "    input, label = torch.tensor(input).to(device).float(), torch.tensor(label).to(device).long()\n",
    "    model.eval()\n",
    "    prediction = model(input).argmax(axis=1)\n",
    "    #print(label)\n",
    "    #print(prediction)\n",
    "    correct = (prediction == label.flatten()).sum().item()\n",
    "    print(f'Accuracy: {correct}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "End of Epoch Loss: 0.112\n",
      "Epoch:  2\n",
      "End of Epoch Loss: 0.062\n",
      "Epoch:  3\n",
      "End of Epoch Loss: 0.016\n",
      "Epoch:  4\n",
      "End of Epoch Loss: 0.003\n",
      "Epoch:  5\n",
      "End of Epoch Loss: 0.001\n",
      "Epoch:  6\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  7\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  8\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  9\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  10\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  11\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  12\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  13\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  14\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  15\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  16\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  17\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  18\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  19\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  20\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  21\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  22\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  23\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  24\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  25\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  26\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  27\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  28\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  29\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  30\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  31\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  32\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  33\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  34\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  35\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  36\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  37\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  38\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  39\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  40\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  41\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  42\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  43\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  44\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  45\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  46\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  47\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  48\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  49\n",
      "End of Epoch Loss: 0.0\n",
      "Epoch:  50\n",
      "End of Epoch Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Train/Test Script\n",
    "for epoch in range(1,51):\n",
    "    print('Epoch: ', epoch)\n",
    "    train(train_data, train_label, model_d, 32, loss_f, optim, device)\n",
    "    #test(valid_data, valid_label, model_d, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, model, device):\n",
    "    model = model.to(device)\n",
    "    input = torch.tensor(input).to(device).float()\n",
    "    model.eval()\n",
    "    prediction = model(input).argmax(axis=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_data(X, symptom_set):\n",
    "    N,d = X.shape\n",
    "    symptoms = X[:,1:]\n",
    "    test_input = np.zeros(shape=(N,131), dtype=int)\n",
    "    #print(symptom_set)\n",
    "    for i in range(N):\n",
    "        for sym in symptoms[i,:]:\n",
    "            if sym != '':\n",
    "                s = sym\n",
    "                if s[0] == ' ':\n",
    "                    s = s[1:]\n",
    "                test_input[i,symptom_set.index(s)] = 1\n",
    "    return test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_label(y, disease_set):\n",
    "    N, = y.shape\n",
    "    to_csv = np.empty(shape=(N+1,2),dtype=object)\n",
    "    to_csv[0] = ['ID','Disease']\n",
    "    for i in range(N):\n",
    "        to_csv[i+1] = [str(i+1), disease_set[y[i]]]\n",
    "        \n",
    "    return to_csv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.loadtxt('./test.csv', delimiter=',', dtype=str, skiprows=1)\n",
    "test_input = transform_test_data(test_data, symp)\n",
    "test_pred = predict(test_input, model_d, device)\n",
    "csv_arr = transform_test_label(test_pred, dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save csv\n",
    "np.savetxt('s21.csv',csv_arr, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "torch.save(model_d, 'cs145_m21.pt')\n",
    "torch.save(model_d.state_dict(), 'cs145_s21.pt')\n",
    "#ms = torch.jit.script(model_d)\n",
    "#ms.save('cs145_ms20.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RecursiveScriptModule:\n\tsize mismatch for ll2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for ll2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for ll5.weight: copying a param with shape torch.Size([41, 128]) from checkpoint, the shape in current model is torch.Size([41, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\OneDrive\\Desktop\\UCLA\\CS 145\\model.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/OneDrive/Desktop/UCLA/CS%20145/model.ipynb#ch0000020?line=0'>1</a>\u001b[0m \u001b[39m#load weights\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/OneDrive/Desktop/UCLA/CS%20145/model.ipynb#ch0000020?line=1'>2</a>\u001b[0m model_d\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mcs145_s19.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mdevice))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/OneDrive/Desktop/UCLA/CS%20145/model.ipynb#ch0000020?line=2'>3</a>\u001b[0m \u001b[39m#model_d = torch.jit.load('cs145_ms20.pt')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/OneDrive/Desktop/UCLA/CS%20145/model.ipynb#ch0000020?line=3'>4</a>\u001b[0m model_d\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\brand\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1491'>1492</a>\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1492'>1493</a>\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1493'>1494</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1495'>1496</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1496'>1497</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1497'>1498</a>\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   <a href='file:///c%3A/Users/brand/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1498'>1499</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RecursiveScriptModule:\n\tsize mismatch for ll2.weight: copying a param with shape torch.Size([128, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for ll2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for ll5.weight: copying a param with shape torch.Size([41, 128]) from checkpoint, the shape in current model is torch.Size([41, 256])."
     ]
    }
   ],
   "source": [
    "#load weights\n",
    "model_d.load_state_dict(torch.load(\"cs145_s19.pt\", map_location=device))\n",
    "#model_d = torch.jit.load('cs145_ms20.pt')\n",
    "model_d.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0374,  0.1373, -0.0719,  ...,  0.0246, -0.0716,  0.0705],\n",
      "        [-0.0177, -0.1162, -0.0045,  ..., -0.0850, -0.0866,  0.0343],\n",
      "        [ 0.1321,  0.0481, -0.0331,  ...,  0.0910, -0.0877, -0.1285],\n",
      "        ...,\n",
      "        [-0.1032, -0.0483, -0.1489,  ...,  0.0614, -0.0406,  0.0097],\n",
      "        [-0.1133,  0.0761,  0.0632,  ...,  0.0438,  0.0860,  0.0048],\n",
      "        [-0.1348, -0.0324,  0.0314,  ...,  0.0782,  0.1394,  0.0767]])\n",
      "tensor([ 1.0674e-01,  9.0635e-02,  1.2956e-01,  8.5871e-02,  4.2702e-02,\n",
      "         9.8036e-02,  9.3130e-02,  1.8881e-01,  7.3198e-02,  1.3612e-01,\n",
      "         9.0414e-04, -1.5010e-03,  1.5433e-01,  1.4147e-01,  1.3897e-01,\n",
      "         1.8595e-01,  1.1150e-01,  3.7593e-02,  1.6711e-02,  6.2511e-02,\n",
      "         1.6203e-01,  1.1519e-01,  1.4757e-01, -1.1442e-02,  1.2283e-01,\n",
      "         1.4929e-01,  3.5682e-02,  1.2927e-01,  1.4095e-01,  7.9222e-02,\n",
      "         1.5836e-01,  5.3494e-02,  1.1332e-01,  7.2772e-02,  1.0601e-01,\n",
      "         5.8297e-02,  1.0291e-01,  1.8111e-01,  1.3357e-01,  2.1790e-02,\n",
      "         1.9217e-01,  1.5540e-01,  9.7635e-03,  1.4326e-02,  4.4459e-02,\n",
      "         4.9553e-02,  7.1959e-02,  1.4193e-01,  1.2720e-01,  9.0424e-02,\n",
      "         2.2032e-02,  1.5022e-01,  1.9560e-01,  2.7329e-02,  6.7483e-02,\n",
      "         1.1277e-01,  2.8955e-02,  1.0634e-01,  7.2266e-02,  1.4378e-01,\n",
      "         3.5659e-02,  1.0026e-02,  7.5421e-02,  9.7735e-02,  6.6255e-02,\n",
      "         8.4093e-02,  7.8030e-02,  9.6419e-02,  8.1878e-02,  9.5835e-02,\n",
      "         1.3650e-01,  1.5047e-02,  1.2380e-01,  1.4995e-01,  9.9882e-02,\n",
      "         7.6542e-02,  4.9912e-02,  1.7733e-01,  1.8446e-02,  8.2544e-02,\n",
      "         1.5622e-01,  1.1320e-01,  4.1181e-02,  8.0439e-02,  1.8917e-01,\n",
      "         8.6138e-02,  3.4724e-02,  4.4295e-02,  3.4618e-02,  1.5042e-02,\n",
      "         2.8251e-02,  6.2285e-02,  7.2875e-02,  1.4860e-01,  2.8838e-02,\n",
      "         5.6428e-02,  6.0234e-02,  1.3347e-01,  6.4530e-02,  1.3034e-01,\n",
      "         9.8041e-02,  7.6644e-02,  9.6272e-02, -8.5305e-03,  1.1888e-01,\n",
      "         1.5988e-01,  4.5732e-02,  1.1177e-01,  1.2992e-01,  1.5267e-01,\n",
      "         1.8322e-01,  1.3882e-01,  1.0620e-01,  5.3419e-02,  9.5118e-02,\n",
      "         1.8928e-01,  1.2305e-01,  1.4819e-01,  1.3321e-01,  1.8291e-01,\n",
      "         3.7372e-02,  1.1010e-01,  1.1730e-01,  1.4252e-01,  1.0748e-01,\n",
      "         1.5202e-01,  5.4831e-02,  1.6746e-01,  1.1645e-01, -2.2034e-02,\n",
      "         1.6045e-01,  7.2863e-02,  6.5483e-03,  3.6900e-02,  4.5456e-02,\n",
      "         8.9383e-02,  1.1991e-01,  1.1612e-01,  1.1050e-01,  2.3669e-02,\n",
      "         5.7991e-02,  1.1413e-01,  2.0158e-01,  8.5078e-02,  4.7492e-02,\n",
      "         3.7396e-02,  5.4426e-02,  1.2659e-01,  1.7536e-01,  1.6227e-01,\n",
      "         4.3156e-02,  1.1921e-01,  1.4987e-01,  5.7670e-02,  1.1614e-01,\n",
      "         1.2276e-01,  1.1984e-02,  1.2985e-01,  1.6916e-01,  1.1605e-01,\n",
      "         1.0111e-01,  1.6278e-02,  8.9917e-02,  1.6594e-01,  9.2082e-02,\n",
      "         1.7135e-02,  1.2597e-01,  1.4098e-01,  1.0489e-01,  1.6334e-01,\n",
      "         1.2426e-01,  1.3042e-01,  1.6195e-01,  1.6059e-04,  1.3087e-01,\n",
      "         1.1936e-01,  1.0191e-01,  1.2582e-01,  1.5239e-01,  7.5379e-02,\n",
      "         1.4084e-01,  9.0390e-02, -1.1898e-02,  7.8993e-02,  6.4862e-02,\n",
      "         1.6307e-01,  3.4977e-02,  6.5127e-02,  8.9921e-02,  5.2350e-02,\n",
      "         3.7803e-02,  1.2099e-01,  8.0200e-02,  1.6191e-01,  4.7828e-02,\n",
      "         8.8639e-02,  2.9789e-02,  1.1226e-01,  3.6877e-02,  1.5358e-01,\n",
      "         8.5733e-02,  5.2220e-02,  2.8567e-02,  5.9094e-02,  6.3069e-02,\n",
      "         8.7358e-02,  6.1704e-02,  1.5044e-01,  8.0034e-02,  1.4404e-01,\n",
      "         1.3335e-01,  1.0563e-01,  1.4344e-01,  1.7203e-01,  1.1115e-01,\n",
      "         1.6806e-01,  1.0707e-01,  3.8785e-03,  3.6655e-02,  6.2315e-02,\n",
      "         7.5564e-02,  6.9564e-02,  2.9553e-02,  3.2559e-02,  3.2408e-02,\n",
      "         9.9828e-02,  1.9045e-02, -1.9382e-02,  8.7489e-02,  2.7065e-02,\n",
      "         9.0634e-02,  8.0810e-02,  5.8925e-02,  1.5522e-01,  1.3528e-01,\n",
      "         1.5242e-01,  1.4223e-01,  1.1026e-01,  8.1000e-03,  1.0520e-01,\n",
      "         4.8910e-02,  4.9351e-02,  4.6552e-02,  9.3528e-02,  4.2187e-02,\n",
      "         1.2840e-01,  1.4742e-01,  5.6397e-02,  6.0199e-02,  3.6593e-02,\n",
      "         8.0442e-02,  1.7438e-01,  7.4890e-02,  6.5579e-02,  1.2084e-01,\n",
      "         9.0130e-02,  4.1260e-02,  1.1833e-01,  1.0778e-01,  7.5762e-02,\n",
      "         9.5113e-02,  9.7195e-02,  1.2554e-02,  9.1661e-02,  1.4854e-01,\n",
      "         9.1580e-02,  6.9404e-02,  1.2648e-01,  8.3881e-02,  1.8394e-01,\n",
      "         1.6919e-01,  1.7682e-01,  5.9933e-02,  8.2688e-02,  6.5946e-02,\n",
      "         6.4070e-02,  5.8536e-02,  3.0733e-02,  1.2407e-01,  8.6110e-02,\n",
      "         1.3529e-01,  1.5141e-01,  6.9736e-02,  6.4721e-02,  3.4847e-02,\n",
      "         5.0297e-02,  1.5803e-01,  7.7991e-02,  4.1497e-02,  1.0112e-01,\n",
      "         1.3354e-01,  1.2702e-01,  7.0801e-02,  1.3819e-01,  1.0706e-01,\n",
      "         6.8420e-02,  7.0189e-03,  9.0211e-02,  1.2679e-01,  1.3112e-01,\n",
      "         4.0439e-02,  8.6842e-02,  4.5818e-02,  1.7030e-01,  4.2929e-02,\n",
      "         1.1128e-01,  9.6310e-02,  4.2334e-02,  1.7590e-01,  1.2023e-01,\n",
      "         1.6512e-01,  5.6649e-02,  8.2107e-02,  1.4703e-01,  1.0306e-01,\n",
      "         1.7257e-01,  1.4764e-01,  1.1742e-01,  1.4147e-01,  9.3058e-02,\n",
      "         9.6837e-03,  1.8525e-01,  1.5201e-01,  5.3126e-02,  1.5173e-01,\n",
      "         1.4321e-01,  1.5926e-01,  9.0698e-02,  1.6838e-01,  8.3478e-02,\n",
      "         2.9342e-02,  1.4498e-01,  1.1540e-01,  2.6692e-02,  4.8577e-02,\n",
      "         1.5805e-01,  1.4351e-01,  2.6325e-02,  9.5147e-02, -4.5797e-03,\n",
      "         2.4518e-02,  1.7989e-01,  1.8985e-02,  1.5182e-01,  1.1251e-01,\n",
      "         4.6048e-02,  3.2486e-02,  5.6173e-02,  9.0867e-02,  8.4200e-02,\n",
      "         1.3985e-01,  1.1859e-01,  1.3112e-01,  2.3132e-02,  7.4192e-02,\n",
      "         8.3273e-03,  7.8180e-02,  9.0182e-02,  1.8968e-01,  1.6053e-01,\n",
      "         1.8719e-01,  1.6265e-01,  1.7945e-01,  3.6961e-02,  1.7134e-01,\n",
      "         4.2251e-03,  4.8856e-02,  6.4556e-02,  8.3776e-02,  8.6195e-02,\n",
      "         1.1326e-01,  1.5106e-01,  1.2493e-02,  1.2441e-01,  4.7529e-02,\n",
      "         1.1602e-01,  7.2065e-02,  7.1826e-02,  1.1065e-01,  1.8989e-01,\n",
      "         6.0885e-02,  1.6194e-01,  1.4868e-01,  1.1818e-01,  1.2768e-01,\n",
      "         1.5116e-01,  8.0398e-02,  5.4390e-02,  1.2289e-01,  7.5164e-02,\n",
      "         3.7510e-02,  1.0880e-01,  6.6651e-03,  1.7941e-01,  7.2841e-02,\n",
      "         1.7326e-01,  7.3216e-02,  1.1688e-01,  1.1470e-01,  4.1040e-02,\n",
      "         1.0348e-01,  1.1330e-01,  1.7537e-01,  1.0242e-01,  1.1068e-01,\n",
      "         1.5982e-01,  6.7406e-02,  6.3060e-02,  1.0958e-01,  1.6077e-02,\n",
      "         6.2468e-02,  5.6533e-02,  1.9775e-01,  2.3984e-02,  1.2516e-01,\n",
      "         1.5000e-02,  8.0587e-02,  4.6618e-02,  8.2321e-02,  1.1018e-01,\n",
      "        -1.4982e-02,  3.9105e-02,  2.8046e-03,  1.4885e-01,  5.3246e-02,\n",
      "         1.3784e-01,  6.2434e-02,  5.9063e-02,  1.7724e-01, -8.0850e-03,\n",
      "         1.2922e-01,  6.6520e-02,  4.7183e-02,  1.0435e-01,  1.5529e-01,\n",
      "         1.5954e-01,  1.4162e-01,  1.5522e-01,  2.6881e-02,  3.5715e-02,\n",
      "         1.4883e-01,  1.1800e-01,  4.1346e-02,  1.4942e-01,  6.9048e-02,\n",
      "         1.8460e-01,  1.9066e-01,  3.7108e-02,  6.8994e-02,  7.9137e-02,\n",
      "         4.1906e-02,  3.8023e-02,  8.7794e-02,  1.5008e-01,  2.8010e-02,\n",
      "         1.1056e-01,  1.4550e-01,  1.4399e-01,  8.9346e-02,  2.8151e-02,\n",
      "         4.2100e-02,  1.6617e-01,  7.7076e-02,  1.1072e-01,  8.0068e-02,\n",
      "         1.6982e-01,  1.5588e-01,  1.4024e-01,  1.5960e-01,  1.5611e-01,\n",
      "         6.6628e-02,  6.1851e-02,  6.9024e-02,  7.2795e-02,  4.2870e-02,\n",
      "         7.3956e-02,  3.0510e-02,  1.1756e-01,  6.5022e-02,  8.6276e-02,\n",
      "         1.7098e-01,  1.3116e-02,  6.2579e-02,  3.8934e-02,  1.8183e-01,\n",
      "         1.1920e-01,  6.9427e-02,  1.0669e-01,  2.3322e-02,  1.0714e-01,\n",
      "         7.7311e-02,  2.7959e-02,  7.1784e-02,  1.7155e-01,  1.4565e-01,\n",
      "         1.2325e-01,  1.6181e-01,  4.6278e-02,  1.3404e-01,  6.8212e-02,\n",
      "         1.2697e-01,  8.3866e-02,  1.3004e-01, -2.9419e-03,  1.6677e-01,\n",
      "         7.7547e-02,  1.1877e-01,  1.4432e-01,  1.5122e-01,  1.2572e-01,\n",
      "         4.7238e-02,  7.3249e-02])\n",
      "tensor([[ 0.0238,  0.1155,  0.0059,  ..., -0.1010,  0.0715,  0.0923],\n",
      "        [-0.0481,  0.0638, -0.0576,  ..., -0.0155, -0.0928,  0.0366],\n",
      "        [ 0.0567, -0.1017,  0.0068,  ..., -0.0327, -0.0545, -0.0397],\n",
      "        ...,\n",
      "        [ 0.0940, -0.0286, -0.0861,  ...,  0.0384,  0.0678,  0.0579],\n",
      "        [ 0.0096, -0.0292, -0.0323,  ...,  0.0253,  0.0613,  0.0266],\n",
      "        [-0.0304,  0.0331,  0.0575,  ...,  0.0240, -0.0095,  0.1475]])\n",
      "tensor([ 0.0208, -0.0155,  0.0059,  0.0287,  0.0207,  0.0332, -0.0505,  0.0799,\n",
      "        -0.0138,  0.0082,  0.0303, -0.0500,  0.0699, -0.0170,  0.0140,  0.0281,\n",
      "        -0.0149,  0.0586, -0.0047,  0.0775, -0.0036, -0.0505, -0.0207,  0.0572,\n",
      "         0.0484, -0.0565,  0.0335, -0.0221,  0.0521,  0.0617, -0.0213,  0.0023,\n",
      "         0.0371,  0.0425, -0.0222,  0.0383,  0.0285,  0.0698,  0.0462,  0.0261,\n",
      "        -0.0020,  0.0126,  0.0325, -0.0540,  0.0439,  0.0715, -0.0211, -0.0570,\n",
      "         0.0260,  0.0091,  0.0087,  0.0124, -0.0080,  0.0377,  0.0487,  0.0183,\n",
      "         0.0584,  0.0090,  0.0066,  0.0909,  0.0182,  0.0197,  0.0139,  0.0391,\n",
      "         0.0570, -0.0362,  0.0416,  0.0147,  0.0331, -0.0353, -0.0127,  0.0108,\n",
      "         0.0336,  0.0680, -0.0066,  0.0656,  0.0399,  0.0299,  0.0323,  0.0543,\n",
      "         0.0509,  0.0688,  0.0869,  0.0135,  0.0127, -0.0397,  0.0123,  0.0236,\n",
      "         0.0287, -0.0181,  0.0036,  0.0153, -0.0086, -0.0197,  0.0322,  0.0113,\n",
      "         0.0727,  0.0329,  0.0474, -0.0009,  0.0137,  0.0201,  0.0107, -0.0135,\n",
      "        -0.0070, -0.0031, -0.0172,  0.0672,  0.0078,  0.0467,  0.0691,  0.0592,\n",
      "        -0.0350,  0.0693,  0.0627,  0.0996,  0.0321,  0.0772,  0.0329, -0.0161,\n",
      "         0.0614, -0.0580,  0.0225,  0.0789,  0.0358,  0.0054,  0.0349,  0.0184])\n",
      "tensor([[ 0.0169, -0.0785, -0.1499,  ...,  0.0075,  0.0749, -0.1368],\n",
      "        [ 0.0350,  0.0155, -0.0690,  ..., -0.0081, -0.0707,  0.0557],\n",
      "        [ 0.0122,  0.0379,  0.0849,  ...,  0.1397,  0.0493, -0.0449],\n",
      "        ...,\n",
      "        [-0.1858, -0.1187,  0.0233,  ..., -0.0756,  0.0847,  0.0339],\n",
      "        [-0.0129,  0.0654, -0.0347,  ...,  0.0177,  0.0425, -0.0704],\n",
      "        [-0.0943,  0.0124, -0.1287,  ..., -0.1041, -0.0621,  0.0648]])\n",
      "tensor([ 0.0919, -0.0794, -0.0688,  0.0252,  0.0116,  0.0783, -0.0458,  0.0333,\n",
      "        -0.0284, -0.0371,  0.0405,  0.0519, -0.0767,  0.0427,  0.0749,  0.0273,\n",
      "         0.0568,  0.0862,  0.0021, -0.0680,  0.0849,  0.0076, -0.0528, -0.0115,\n",
      "        -0.0635, -0.0138,  0.0214, -0.0558, -0.0412, -0.0650,  0.0061, -0.0248,\n",
      "        -0.0444,  0.0704, -0.0066,  0.0982, -0.0706, -0.0242,  0.0272,  0.0158,\n",
      "        -0.0818])\n"
     ]
    }
   ],
   "source": [
    "for param in model_d.parameters():\n",
    "        print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0012, -0.0345,  0.0897,  ...,  0.1561, -0.1255,  0.1192],\n",
      "        [ 0.0227,  0.0353, -0.0450,  ..., -0.0114, -0.0044,  0.0367],\n",
      "        [-0.1016,  0.0405,  0.0441,  ..., -0.0188,  0.0172,  0.1058],\n",
      "        ...,\n",
      "        [-0.0671,  0.0205,  0.1105,  ...,  0.0521,  0.0090,  0.0586],\n",
      "        [-0.0170, -0.1160, -0.0456,  ..., -0.1243, -0.0762,  0.1202],\n",
      "        [-0.0392, -0.0541,  0.0588,  ..., -0.1138,  0.0201,  0.0385]])\n",
      "tensor([ 0.1169,  0.1526,  0.1007,  0.1636,  0.1883,  0.1289,  0.1169, -0.0078,\n",
      "         0.0728,  0.0975,  0.1432,  0.0622,  0.1452,  0.0830,  0.1332,  0.1127,\n",
      "         0.1435,  0.0579,  0.0296,  0.1865,  0.0349,  0.0569,  0.0706,  0.0864,\n",
      "         0.0913,  0.0552,  0.0183,  0.0101,  0.0208,  0.0158,  0.0117,  0.0986,\n",
      "         0.0920,  0.0945,  0.0491,  0.0562,  0.1469,  0.0291,  0.0952,  0.1503,\n",
      "        -0.0235,  0.0604,  0.0016,  0.0717,  0.1273,  0.0435,  0.0495, -0.0062,\n",
      "        -0.0103,  0.0377,  0.1608,  0.0493,  0.1070,  0.0577,  0.0981,  0.1493,\n",
      "         0.0346,  0.1344, -0.0124,  0.0721,  0.1148,  0.1672,  0.0902,  0.1002,\n",
      "         0.0909,  0.1025,  0.0799,  0.0907,  0.0799,  0.1491,  0.1916,  0.0112,\n",
      "         0.0798,  0.1381,  0.1130,  0.1144,  0.1483,  0.0011, -0.0063,  0.1718,\n",
      "         0.1282,  0.1193,  0.0426,  0.0859,  0.0806,  0.0689,  0.1380,  0.0996,\n",
      "         0.0649,  0.1109,  0.1333,  0.0585,  0.1427,  0.0511,  0.0668,  0.1385,\n",
      "         0.0135,  0.0820,  0.0915,  0.0595,  0.1657,  0.0574,  0.1606,  0.1958,\n",
      "         0.0097,  0.0174,  0.0648,  0.0397,  0.0511,  0.0967, -0.0255,  0.1674,\n",
      "         0.0852,  0.0964,  0.1095,  0.1771,  0.0130,  0.1178,  0.0537,  0.1127,\n",
      "        -0.0070,  0.0050,  0.0957,  0.0961,  0.1161,  0.1715,  0.1633,  0.1230,\n",
      "         0.0761,  0.1380,  0.0949,  0.0493,  0.1093,  0.1383,  0.1378,  0.1891,\n",
      "         0.1244,  0.1095,  0.1397,  0.0851,  0.1309,  0.0498,  0.1044, -0.0039,\n",
      "         0.1353,  0.1689,  0.1651,  0.0561,  0.0578,  0.0929,  0.1615,  0.0474,\n",
      "         0.1532, -0.0226,  0.0522,  0.1921,  0.1752,  0.0268,  0.1630,  0.0824,\n",
      "         0.1244,  0.0653,  0.0705,  0.1358,  0.0296,  0.1380,  0.0868,  0.0585,\n",
      "         0.1181,  0.1556,  0.0243,  0.1554,  0.0136,  0.1100,  0.0297,  0.0162,\n",
      "         0.1673,  0.1429,  0.1746,  0.1124,  0.0457,  0.0394,  0.0334,  0.1128,\n",
      "         0.0235,  0.1560,  0.0690,  0.0845,  0.1244,  0.0420,  0.1379,  0.0769,\n",
      "         0.1218,  0.1452,  0.1175,  0.0591,  0.0314,  0.1693,  0.0724,  0.1353,\n",
      "         0.0157,  0.0709,  0.0678,  0.1256,  0.0550,  0.1486,  0.1628,  0.1672,\n",
      "         0.0058,  0.0969,  0.0238,  0.0820,  0.0644,  0.0424,  0.1282,  0.0493,\n",
      "         0.0218,  0.1041,  0.0449,  0.0785,  0.1123,  0.0152,  0.1333, -0.0012,\n",
      "         0.1760,  0.1017,  0.1429,  0.0414,  0.1297,  0.0631,  0.0325,  0.1012,\n",
      "         0.1454,  0.0566,  0.1695,  0.0949,  0.0322,  0.0829,  0.1614,  0.0863,\n",
      "         0.1004,  0.0892,  0.0812, -0.0041,  0.0869,  0.1665,  0.0129,  0.0880,\n",
      "         0.0376,  0.0010,  0.0672,  0.1769,  0.0190,  0.0492,  0.1513,  0.1109,\n",
      "         0.1388,  0.1563,  0.1142,  0.0219,  0.1078,  0.1004,  0.0327, -0.0093,\n",
      "         0.1429,  0.0778,  0.1480,  0.1107,  0.0250, -0.0113,  0.1434,  0.0196,\n",
      "         0.0702,  0.1457,  0.0830,  0.0877,  0.0367,  0.1154,  0.1605,  0.0398,\n",
      "         0.0938,  0.0904,  0.0281,  0.1530,  0.1034,  0.0600,  0.0196,  0.1674,\n",
      "         0.1234,  0.1150,  0.1297,  0.1774, -0.0100,  0.1514,  0.0824,  0.0801,\n",
      "         0.0768,  0.0610,  0.0497,  0.1655,  0.1485,  0.1274,  0.0022,  0.1743,\n",
      "         0.1568,  0.1537,  0.1420,  0.0835,  0.0802,  0.1618,  0.0802,  0.1151,\n",
      "         0.1751,  0.0390,  0.0174,  0.1758,  0.0604,  0.0741,  0.0823,  0.1873,\n",
      "         0.1629,  0.0680,  0.0117,  0.0335,  0.0527,  0.1211,  0.1234,  0.1460,\n",
      "         0.1712,  0.1288,  0.1625, -0.0045,  0.1217,  0.0565,  0.1319,  0.1137,\n",
      "         0.1678, -0.0116,  0.0819,  0.0387,  0.1341,  0.0151,  0.0603,  0.0789,\n",
      "         0.0960,  0.0754,  0.1308,  0.1302,  0.0285,  0.1400,  0.0375,  0.0469,\n",
      "         0.1227,  0.0431,  0.0053,  0.0764,  0.0416,  0.0310,  0.0402,  0.1002,\n",
      "         0.0697,  0.0670,  0.0154,  0.0582,  0.1975,  0.0254,  0.1674, -0.0033,\n",
      "         0.0875,  0.0786,  0.1678,  0.1066,  0.0717, -0.0316,  0.1441,  0.1745,\n",
      "         0.0360,  0.1432,  0.0802,  0.1343, -0.0063,  0.0617,  0.0047,  0.0483,\n",
      "         0.0525,  0.1104,  0.0292,  0.0697,  0.1450,  0.1497,  0.0708,  0.1518,\n",
      "         0.0938,  0.0995,  0.1541,  0.1155,  0.0936,  0.1329,  0.0752,  0.0064,\n",
      "         0.0284,  0.0752,  0.1473,  0.0470,  0.0723,  0.1098,  0.1199,  0.0411,\n",
      "         0.0277,  0.1384,  0.0447,  0.0597,  0.0329,  0.0733,  0.0486,  0.1689,\n",
      "         0.0373,  0.0431,  0.0501,  0.1430,  0.0736,  0.0916,  0.1119,  0.0500,\n",
      "         0.1283,  0.1516,  0.1114,  0.0601,  0.0950, -0.0167,  0.0518,  0.0471,\n",
      "         0.1016,  0.1349,  0.1357,  0.1129,  0.0284,  0.0320,  0.1326,  0.0337,\n",
      "         0.1512,  0.1803,  0.0967,  0.1435,  0.0517,  0.1512,  0.1133,  0.1541,\n",
      "         0.0316,  0.0456,  0.0370,  0.0128,  0.0917,  0.0535,  0.0475,  0.0891,\n",
      "         0.1159,  0.0950,  0.1792,  0.0718,  0.0318,  0.0146,  0.1462,  0.1443,\n",
      "         0.0177,  0.0417,  0.1856,  0.1316, -0.0170,  0.1509,  0.0741,  0.0976,\n",
      "         0.1805,  0.0972,  0.0452,  0.0318,  0.1398,  0.0634,  0.1037,  0.0616,\n",
      "         0.1579,  0.0167,  0.1193,  0.0140,  0.1078,  0.0581,  0.0530,  0.0496,\n",
      "         0.1183,  0.0601,  0.0473,  0.1091, -0.0093,  0.0756,  0.0929,  0.0483,\n",
      "         0.0860,  0.0876,  0.0729,  0.0129,  0.1127,  0.0981,  0.0737,  0.1338,\n",
      "         0.1031,  0.0480,  0.1373,  0.1015,  0.0993,  0.1476,  0.0229, -0.0211])\n",
      "tensor([[-0.0126,  0.0018, -0.0455,  ...,  0.0209, -0.1261, -0.0500],\n",
      "        [ 0.0531, -0.0100,  0.0399,  ..., -0.0471, -0.0626,  0.0779],\n",
      "        [-0.0291,  0.0201, -0.0883,  ..., -0.0170,  0.0766,  0.0104],\n",
      "        ...,\n",
      "        [ 0.0204,  0.0930, -0.0657,  ..., -0.0225,  0.0301, -0.0108],\n",
      "        [ 0.0643, -0.0729,  0.1046,  ...,  0.1246,  0.0604,  0.0361],\n",
      "        [ 0.0052, -0.0515,  0.0235,  ..., -0.0150, -0.0516, -0.0430]])\n",
      "tensor([ 4.8748e-02, -2.6896e-02,  7.6368e-03, -9.5884e-03,  7.2233e-02,\n",
      "        -1.9297e-02,  8.5026e-02,  5.4810e-02,  3.2715e-02,  3.1547e-02,\n",
      "        -2.2254e-02,  2.5415e-02, -9.3640e-03,  9.0142e-02,  3.9805e-02,\n",
      "         2.8195e-02,  5.2677e-02,  7.6956e-02,  5.0047e-02,  3.4302e-02,\n",
      "         1.8607e-02,  9.1588e-03,  1.7579e-02,  2.1748e-02, -4.6772e-02,\n",
      "        -5.1725e-03,  3.6993e-02,  3.0060e-02,  2.9635e-02, -2.6460e-02,\n",
      "         6.9488e-02,  7.2698e-04, -5.5869e-02,  4.3576e-02, -7.5738e-03,\n",
      "         5.1283e-02, -4.7994e-02, -2.1940e-02,  7.5731e-03,  1.3355e-02,\n",
      "         3.2052e-02,  1.1591e-02, -3.6630e-02,  3.7444e-02,  4.2254e-02,\n",
      "         2.4347e-03, -5.8192e-02,  6.1499e-02,  2.6475e-02,  5.9365e-02,\n",
      "         2.5463e-02,  6.1154e-02,  2.5266e-02,  1.9729e-02,  3.1340e-02,\n",
      "         5.5188e-02,  4.5769e-02, -5.1132e-02,  2.0543e-02,  5.3758e-02,\n",
      "        -2.1095e-02, -1.1567e-02, -1.3590e-02,  2.7762e-02,  1.1557e-02,\n",
      "         1.1180e-02, -8.7324e-03, -8.7475e-03, -1.5969e-02, -7.0206e-03,\n",
      "         7.5842e-02,  1.7644e-04,  2.3607e-02,  2.5879e-02,  2.0721e-02,\n",
      "         4.2915e-02,  2.5744e-02, -2.4978e-02, -2.8617e-03,  2.7802e-02,\n",
      "        -1.5495e-03, -2.2984e-02,  6.9345e-02,  1.0096e-02,  5.2318e-02,\n",
      "        -4.1350e-02,  6.2542e-02,  1.2316e-02,  4.7938e-02, -3.9099e-02,\n",
      "         2.2071e-02,  7.2951e-03,  7.0563e-02, -5.6861e-04,  2.8479e-02,\n",
      "         6.3243e-02,  6.8623e-02,  2.0851e-02,  4.3509e-02, -3.4655e-02,\n",
      "        -2.1012e-02,  5.5528e-02, -6.1321e-03,  1.0023e-01,  1.9055e-02,\n",
      "         5.8579e-02, -3.7884e-03,  2.2160e-02,  5.5707e-02,  4.5617e-02,\n",
      "        -2.1340e-02, -2.4102e-02,  8.8030e-05,  8.8061e-02, -1.6526e-02,\n",
      "         3.7808e-02,  3.9460e-02, -1.6019e-02,  5.1942e-02, -3.5919e-02,\n",
      "        -4.4302e-02,  8.5185e-02, -1.2508e-02, -3.2140e-02,  6.3904e-02,\n",
      "         2.2144e-02, -8.8267e-03, -3.7213e-02,  3.8153e-03, -3.6135e-02,\n",
      "         5.6265e-02,  6.0302e-02, -5.8973e-02,  4.5245e-02,  1.8288e-02,\n",
      "         5.1849e-02, -5.6873e-02, -2.1101e-02,  2.9670e-02,  8.0293e-02,\n",
      "         2.2071e-02,  1.0806e-02,  2.2334e-03, -1.2925e-02, -1.0070e-02,\n",
      "        -8.5997e-03,  7.9995e-03,  2.6934e-03, -1.2130e-02,  3.3446e-02,\n",
      "         3.6283e-02, -3.6389e-02, -5.6296e-02,  6.7927e-02,  8.6189e-02,\n",
      "         3.4583e-02,  3.9329e-03, -3.7749e-02,  2.4940e-02,  2.2950e-02,\n",
      "         4.9066e-02, -2.2329e-03,  4.3215e-02,  6.5433e-03,  2.2710e-02,\n",
      "         3.1375e-02, -5.7279e-02,  8.1326e-02,  1.8535e-02,  2.5635e-02,\n",
      "         2.0038e-02,  2.2454e-02, -4.6585e-03,  7.2879e-02,  9.3459e-05,\n",
      "         3.3127e-02,  3.4223e-02,  4.4639e-02,  5.9714e-03,  4.5186e-02,\n",
      "         1.3842e-02,  2.5195e-02,  2.1747e-02,  2.1095e-02,  3.8500e-02,\n",
      "         7.1753e-02,  6.7609e-02, -1.5062e-02,  5.1333e-02,  3.1096e-02,\n",
      "        -2.2173e-02, -5.3484e-02, -5.5608e-02,  1.8857e-02,  1.6971e-02,\n",
      "         3.3145e-02,  5.4191e-02,  5.8911e-03, -6.8554e-03, -3.5413e-02,\n",
      "         3.2522e-02,  2.0002e-02, -3.3394e-02,  1.3948e-02, -4.8968e-02,\n",
      "         7.0343e-02, -2.0011e-02,  2.4486e-02,  2.4229e-02,  3.6099e-03,\n",
      "         1.7526e-02,  1.7600e-02,  5.3906e-02,  4.8633e-03,  6.4596e-02,\n",
      "        -1.3134e-02,  4.5418e-02,  2.1819e-02, -1.0995e-02, -3.3936e-02,\n",
      "         4.1760e-02, -1.1293e-02, -3.0896e-02,  4.0536e-02,  2.6067e-02,\n",
      "        -4.8936e-02, -2.7974e-02,  3.1873e-02,  2.7160e-02, -2.5782e-02,\n",
      "         4.6506e-03,  4.1655e-02,  4.0110e-02,  3.0267e-02,  2.6844e-02,\n",
      "         8.1789e-02, -2.2398e-02,  8.0512e-02, -1.8377e-02, -3.1001e-02,\n",
      "        -1.4147e-03, -4.3254e-02, -4.3420e-02,  4.9400e-02,  4.4928e-02,\n",
      "        -5.3973e-03,  5.3388e-03,  3.9941e-02,  6.6252e-03, -5.2719e-03,\n",
      "        -3.8425e-02,  3.0817e-03,  3.2177e-02,  4.4328e-02,  4.6438e-02,\n",
      "        -5.7978e-02])\n",
      "tensor([[-0.0491, -0.0834, -0.0284,  ..., -0.0737,  0.0660, -0.0029],\n",
      "        [-0.2159,  0.0049,  0.0808,  ..., -0.0887,  0.0141, -0.0237],\n",
      "        [ 0.0185, -0.1713, -0.0172,  ..., -0.0653,  0.0841,  0.0290],\n",
      "        ...,\n",
      "        [-0.0964, -0.0981, -0.0950,  ...,  0.0344,  0.0108, -0.0254],\n",
      "        [ 0.0304,  0.1145,  0.0873,  ..., -0.1454, -0.0354, -0.0144],\n",
      "        [-0.0446, -0.0718,  0.0680,  ...,  0.0469, -0.1400, -0.0249]])\n",
      "tensor([ 0.0445, -0.0046, -0.0097,  0.0714,  0.0656,  0.0507, -0.0517, -0.0539,\n",
      "         0.0185, -0.0680, -0.0513,  0.0543,  0.0075, -0.0277, -0.0023,  0.0015,\n",
      "        -0.0183, -0.0418,  0.0448, -0.0302, -0.0077, -0.0523, -0.0148,  0.0167,\n",
      "        -0.0560, -0.0791,  0.0383,  0.0501, -0.0365, -0.0340,  0.0263,  0.0174,\n",
      "        -0.0026, -0.0184,  0.0104, -0.0433,  0.0220, -0.0536,  0.0092, -0.0056,\n",
      "        -0.0262])\n"
     ]
    }
   ],
   "source": [
    "for param in model_d.parameters():\n",
    "        print(param.data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69a9235b2799f09bc7a4d7fc4018927df298a0a697379818c8dec9478f72590e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
